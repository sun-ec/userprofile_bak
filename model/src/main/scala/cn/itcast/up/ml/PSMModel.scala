package cn.itcast.up.ml

import cn.itcast.up.base.BaseModel
import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.sql.{DataFrame, functions}

/**
  * Author itcast
  * Date 2019/11/23 14:38
  * Desc 使用KMeans + PSM 完成价格敏感度模型Price Sensitivity Meter
  * 有时在实际业务中，会把用户分为3-5类，
  * 比如分为极度敏感、较敏感、一般敏感、较不敏感、极度不敏感。
  * 然后将每类的聚类中心值与实际业务所需的其他指标结合，最终确定人群类别，判断在不同需求下是否触达或怎样触达。
  * 比如电商要通过满减优惠推广一新品牌的麦片，
  * 此时可优先选择优惠敏感且对麦片有消费偏好的用户进行精准推送，
  * 至于优惠敏感但日常对麦片无偏好的用户可暂时不进行推送或减小推送力度，
  * 优惠不敏感且对麦片无偏好的用户可选择不进行推送。
  * 可见，在实际操作中，技术指标评价外，还应结合业务需要，才能使模型达到理想效果。
  * psm = 优惠订单占比 + 平均优惠金额占比 + 优惠总金额占比
  * psm = 优惠订单数/总订单数 + 平均优惠金额/平均每单应收金额 + 优惠总金额/应收总金额
  * psm = 优惠订单数/总订单数 + ((优惠总金额/优惠订单数)/(应收总金额/总订单数)) + 优惠总金额/应收总金额
  * 需要算: 优惠订单数、总订单数、优惠总金额、应收总金额  //优惠订单数可能为0
  */
object PSMModel extends BaseModel {
  def main(args: Array[String]): Unit = {
    execute()
  }
  /**
    * 提供一个抽象方法由子类实现并返回标签id
    * @return
    */
  override def getTagID(): Int = 50

  /**
    * 标签计算的具体流程,应该由子类去实现
    * @param fiveRule
    * @param HBaseDF
    * @return
    */
  override def compute(fiveRule: DataFrame, HBaseDF: DataFrame): DataFrame = {
    //fiveRule.show(10,false)
    //fiveRule.printSchema()
    //HBaseDF.show(10,false)
    //HBaseDF.printSchema()
    /*
+---+----+
|id |rule|
+---+----+
|51 |1   |
|52 |2   |
|53 |3   |
|54 |4   |
|55 |5   |
+---+----+

root
 |-- id: long (nullable = false)
 |-- rule: string (nullable = true)

+---------+-------------------+-----------+---------------+
|memberId |orderSn            |orderAmount|couponCodeValue|
+---------+-------------------+-----------+---------------+
|13823431 |ts_792756751164275 |2479.45    |0.00           |
|4035167  |D14090106121770839 |2449.00    |0.00           |
|4035291  |D14090112394810659 |1099.42    |0.00           |
|4035041  |fx_787749561729045 |1999.00    |0.00           |
|13823285 |D14092120154435903 |2488.00    |0.00           |
|4034219  |D14092120155620305 |3449.00    |0.00           |
|138230939|top_810791455519102|1649.00    |0.00           |
|4035083  |D14092120161884409 |7.00       |0.00           |
|138230935|D14092120162313538 |1299.00    |0.00           |
|13823231 |D14092120162378713 |499.00     |0.00           |
+---------+-------------------+-----------+---------------+
only showing top 10 rows

root
 |-- memberId: string (nullable = true)
 |-- orderSn: string (nullable = true)
 |-- orderAmount: string (nullable = true)
 |-- couponCodeValue: string (nullable = true)
     */

    //0.导入隐式转换
    import org.apache.spark.sql.functions._
    import spark.implicits._

    //0.字符串常量
    val psmScoreStr: String = "psm"
    val featureStr: String = "feature"
    val predictStr: String = "predict"

    //1.计算：应收金额/优惠金额/实收金额/订单状态
    //ra:receivableAmount应收金额  100
    //da:discountAmount 优惠金额   20
    //pa:practicalAmount 实收金额   80
    //state:订单状态,优惠金额不为0的才为优惠订单,记为1
    val raColumn = 'orderAmount + 'couponCodeValue as "ra"
    val daColumn = 'couponCodeValue as "da"
    val paColumn = 'orderAmount as "pa"
    val stateColumn= functions
      .when('couponCodeValue =!= 0.0d,1) //优惠金额不为0的才为优惠订单,记为1
      .when('couponCodeValue === 0.0d,0)
      .as("state")

    val tempDF: DataFrame = HBaseDF.select('memberId as "userId",raColumn,daColumn,paColumn,stateColumn)
    //tempDF.show(10,false)
    //tempDF.printSchema()
    /*
 +---------+-------+----+-------+-----+
|userId   |ra应收 |da优惠|pa实收 |state|
+---------+-------+----+-------+-----+
|13823431 |2479.45|0.00|2479.45|0    |
|4035167  |2449.0 |0.00|2449.00|0    |
|4035291  |1099.42|0.00|1099.42|0    |
|4035041  |1999.0 |0.00|1999.00|0    |
|13823285 |2488.0 |0.00|2488.00|0    |
|4034219  |3449.0 |0.00|3449.00|0    |
|138230939|1649.0 |0.00|1649.00|0    |
|4035083  |7.0    |0.00|7.00   |0    |
|138230935|1299.0 |0.00|1299.00|0    |
|13823231 |499.0  |0.00|499.00 |0    |
+---------+-------+----+-------+-----+
only showing top 10 rows

root
 |-- userId: string (nullable = true)
 |-- ra: double (nullable = true)
 |-- da: string (nullable = true)
 |-- pa: string (nullable = true)
 |-- state: integer (nullable = true)
     */

    //2.计算:优惠订单数/总订单数/优惠总金额/应收总金额
    //tdon :total  discount order num 优惠订单数 (优惠金额不为0的才为优惠订单)
    //ton:  total  order num 总订单数
    //tda :total  discountAmount 优惠总金额
    //tra :total  receivableAmount 应收总金额
    val tdonColumn = sum('state) as "tdon"
    val tonColumn = count('state) as "ton"
    val tdaColumn = sum('da) as "tda"
    val traColumn = sum('ra) as "tra"

    val tempDF2: DataFrame = tempDF.groupBy('userId)
      .agg(tdonColumn, tonColumn, tdaColumn, traColumn)
    ///tempDF2.show(20,false)
/*
       优惠订单数/总订单数/优惠总金额/应收总金额
+---------+----+---+------+------------------+
|userId   |tdon|ton|tda   |tra               |
+---------+----+---+------+------------------+
|4033473  |3   |142|500.0 |252430.92         |
|13822725 |4   |116|800.0 |180098.34         |
|13823681 |1   |108|200.0 |169946.1          |
|138230919|3   |125|600.0 |240661.56999999998|
|13823083 |3   |132|600.0 |234124.17         |
|13823431 |2   |122|400.0 |181258.22         |
|4034923  |1   |108|200.0 |167674.89         |
|4033575  |4   |125|650.0 |255866.40000000002|
|13822841 |0   |113|0.0   |205931.91         |
|13823153 |6   |133|1200.0|251898.57         |
|4034191  |4   |111|800.0 |309843.99         |
|4033483  |2   |110|400.0 |158211.09999999998|
|4033348  |2   |145|400.0 |240573.78999999998|
|4034761  |6   |138|950.0 |260077.91         |
|4035131  |2   |113|1100.0|169220.14         |
|13823077 |1   |126|200.0 |208607.90999999997|
|138230937|1   |104|200.0 |147354.71         |
|4034641  |5   |133|1000.0|246532.28         |
|7        |3   |134|600.0 |251617.11000000002|
|138230911|1   |123|200.0 |225214.42         |
+---------+----+---+------+------------------+
 */
    //3.计算:psm
    //psm = 优惠订单占比 + 平均优惠金额占比 + 优惠总金额占比
    //psm = 优惠订单数/总订单数 + 平均优惠金额/平均每单应收金额 + 优惠总金额/应收总金额
    //psm = 优惠订单数/总订单数 + ((优惠总金额/优惠订单数)/(应收总金额/总订单数)) + 优惠总金额/应收总金额
    //需要算: 优惠订单数、总订单数、优惠总金额、应收总金额  //优惠订单数可能为0
    val psmColumn = 'tdon / 'ton  + (('tda / 'tdon) / ('tra / 'ton)) +  'tda / 'tra as "psm"
    val psmScoreDF: DataFrame = tempDF2.select('userId,psmColumn).filter('psm.isNotNull)
    //psm的计算有很多除法,除数有可能为0,SparkSQL对于除数为0的记录直接返回null
    //注意:SparkMLlib在计算的时候不能有null记录,所以应该将null记录过滤掉
    //注意:SparkSQL的DSL语法中对于Null值的判断得使用isNotNull方法
    //用"null" null 都不行
    //注意:对于tdon优惠订单数为0,我们这里演示的知识点是机器学习需要处理null值,及如何处理
    //而对于tdon优惠订单数为0的用户实际上是对价格不敏感的用户,应该要保留
    //那么就可以将tdon优惠订单数为0的用户的用户的tdon优惠订单数置为一个很小的值,如0.00001
    psmScoreDF.show(10,false)
    /*
+---------+-------------------+
|userId   |psm                |
+---------+-------------------+
|4033473  |0.11686252330855691|
|13822725 |0.16774328728519597|
|13823681 |0.13753522440350205|
|138230919|0.1303734438365045 |
|13823083 |0.1380506927739941 |
|13823431 |0.15321482374431458|
|4034923  |0.13927276336831218|
|4033575  |0.11392752155030905|
|13823153 |0.15547466292943982|
|4034191  |0.11026694172505715|
+---------+-------------------+
     */

    //4.特征向量化
    val VectorDF: DataFrame = new VectorAssembler()
      .setInputCols(Array(psmScoreStr))
      .setOutputCol(featureStr)
      .transform(psmScoreDF)
    VectorDF.show(10,false)
/*
+---------+-------------------+---------------------+
|userId   |psm                |feature              |
+---------+-------------------+---------------------+
|4033473  |0.11686252330855691|[0.11686252330855691]|
|13822725 |0.16774328728519597|[0.16774328728519597]|
|13823681 |0.13753522440350205|[0.13753522440350205]|
|138230919|0.1303734438365045 |[0.1303734438365045] |
|13823083 |0.1380506927739941 |[0.1380506927739941] |
|13823431 |0.15321482374431458|[0.15321482374431458]|
|4034923  |0.13927276336831218|[0.13927276336831218]|
|4033575  |0.11392752155030905|[0.11392752155030905]|
|13823153 |0.15547466292943982|[0.15547466292943982]|
|4034191  |0.11026694172505715|[0.11026694172505715]|
+---------+-------------------+---------------------+
 */
    //5.创建KMeans模型并训练
    val model: KMeansModel = new KMeans()
      .setFeaturesCol(featureStr)
      .setPredictionCol(predictStr)
      .setK(5) //实际中应该由肘部法则和运营策略结合选取最合适的K值
      .setMaxIter(10)
      .setSeed(10)
      .fit(VectorDF)

    //6.预测/聚类
    val clusterDF: DataFrame = model.transform(VectorDF)
    clusterDF.show(10,false)
    /*
+---------+-------------------+---------------------+-------+
|userId   |psm                |feature              |predict|
+---------+-------------------+---------------------+-------+
|4033473  |0.11686252330855691|[0.11686252330855691]|0      |
|13822725 |0.16774328728519597|[0.16774328728519597]|0      |
|13823681 |0.13753522440350205|[0.13753522440350205]|0      |
|138230919|0.1303734438365045 |[0.1303734438365045] |0      |
|13823083 |0.1380506927739941 |[0.1380506927739941] |0      |
|13823431 |0.15321482374431458|[0.15321482374431458]|0      |
|4034923  |0.13927276336831218|[0.13927276336831218]|0      |
|4033575  |0.11392752155030905|[0.11392752155030905]|0      |
|13823153 |0.15547466292943982|[0.15547466292943982]|0      |
|4034191  |0.11026694172505715|[0.11026694172505715]|0      |
+---------+-------------------+---------------------+-------+
     */

    //7.查看每个聚类的PSM的最小/最大值
    clusterDF.groupBy('predict)
      .agg(max('psm),min('psm))
      .orderBy('predict)
      .show(false)
    /*
+-------+-------------------+--------------------+
|predict|max(psm)           |min(psm)            |
+-------+-------------------+--------------------+
|0      |0.17212111010889647|0.10813574000787066 |
|1      |0.6476100325971383 |0.4529572968183633  |
|2      |0.40727743554668544|0.26608282379843023 |
|3      |0.10774876146242558|0.023222599249744437|
|4      |0.2618558914500962 |0.17424644824695665 |
+-------+-------------------+--------------------+
     */

    //8.拉链
    val newDF: DataFrame = zipResult(fiveRule, model, clusterDF)
    newDF
  }
}
